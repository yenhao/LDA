{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim import corpora, models\n",
    "import gensim\n",
    "from pprint import pprint\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# create English stop words list\n",
    "en_stop = get_stop_words('en')\n",
    "\n",
    "# Create p_stemmer of class PorterStemmer\n",
    "p_stemmer = PorterStemmer()\n",
    "    \n",
    "# create sample documents\n",
    "# doc_a = \"Brocolli is good to eat. My brother likes to eat good brocolli, but not my mother.\"\n",
    "# doc_b = \"My mother spends a lot of time driving my brother around to baseball practice.\"\n",
    "# doc_c = \"Some health experts suggest that driving may cause increased tension and blood pressure.\"\n",
    "# doc_d = \"I often feel pressure to perform well at school, but my mother never seems to drive my brother to do better.\"\n",
    "# doc_e = \"Health professionals say that brocolli is good for your health.\" \n",
    "doc_f = \"I eat fish and vegetables.\"\n",
    "doc_g = \"Dog and fish are pets.\"\n",
    "doc_h = \"My kitten eats fish.\"\n",
    "# doc_i = \"Fish Fish Eat Eat Vegetables\"\n",
    "# doc_j = \"Fish Fish Milk Kitten Kitten\"\n",
    "# compile sample documents into a list\n",
    "# doc_set = [doc_a, doc_b, doc_c, doc_d, doc_e]\n",
    "doc_set = [doc_f, doc_g, doc_h]\n",
    "# doc_set = [doc_i, doc_j]\n",
    "\n",
    "# list for tokenized documents in loop\n",
    "texts = []\n",
    "\n",
    "# loop through document list\n",
    "for i in doc_set:\n",
    "    \n",
    "    # clean and tokenize document string\n",
    "    raw = i.lower()\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "\n",
    "    # remove stop words from tokens\n",
    "    stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "    \n",
    "    # stem tokens\n",
    "    stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "    \n",
    "    # add tokens to list\n",
    "    texts.append(stemmed_tokens)\n",
    "\n",
    "# turn our tokenized documents into a id <-> term dictionary\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "    \n",
    "# convert tokenized documents into a document-term matrix\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "# generate LDA model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'running online LDA training, 2 topics, 20 passes over the supplied corpus of 3 documents, updating model once every 3 documents, evaluating perplexity every 3 documents, iterating 50x with a convergence threshold of 0.001'\n",
      "0\n",
      "[(0, u'0.287*\"fish\" + 0.177*\"eat\" + 0.153*\"pet\" + 0.152*\"dog\"'),\n",
      " (1, u'0.240*\"fish\" + 0.230*\"eat\" + 0.201*\"veget\" + 0.111*\"kitten\"')]\n",
      "1\n",
      "[(0, u'0.285*\"fish\" + 0.167*\"eat\" + 0.161*\"pet\" + 0.160*\"dog\"'),\n",
      " (1, u'0.245*\"eat\" + 0.242*\"fish\" + 0.220*\"veget\" + 0.101*\"kitten\"')]\n",
      "2\n",
      "[(0, u'0.283*\"fish\" + 0.165*\"pet\" + 0.165*\"dog\" + 0.161*\"kitten\"'),\n",
      " (1, u'0.254*\"eat\" + 0.244*\"fish\" + 0.228*\"veget\" + 0.095*\"kitten\"')]\n",
      "3\n",
      "[(0, u'0.282*\"fish\" + 0.168*\"pet\" + 0.168*\"dog\" + 0.163*\"kitten\"'),\n",
      " (1, u'0.260*\"eat\" + 0.245*\"fish\" + 0.231*\"veget\" + 0.092*\"kitten\"')]\n",
      "4\n",
      "[(0, u'0.282*\"fish\" + 0.170*\"pet\" + 0.170*\"dog\" + 0.165*\"kitten\"'),\n",
      " (1, u'0.265*\"eat\" + 0.246*\"fish\" + 0.231*\"veget\" + 0.090*\"kitten\"')]\n",
      "5\n",
      "[(0, u'0.281*\"fish\" + 0.172*\"pet\" + 0.172*\"dog\" + 0.166*\"kitten\"'),\n",
      " (1, u'0.269*\"eat\" + 0.247*\"fish\" + 0.230*\"veget\" + 0.089*\"kitten\"')]\n",
      "6\n",
      "[(0, u'0.281*\"fish\" + 0.173*\"pet\" + 0.173*\"dog\" + 0.167*\"kitten\"'),\n",
      " (1, u'0.273*\"eat\" + 0.248*\"fish\" + 0.229*\"veget\" + 0.089*\"kitten\"')]\n",
      "7\n",
      "[(0, u'0.281*\"fish\" + 0.175*\"pet\" + 0.175*\"dog\" + 0.168*\"kitten\"'),\n",
      " (1, u'0.277*\"eat\" + 0.249*\"fish\" + 0.227*\"veget\" + 0.088*\"kitten\"')]\n",
      "8\n",
      "[(0, u'0.280*\"fish\" + 0.177*\"pet\" + 0.177*\"dog\" + 0.169*\"kitten\"'),\n",
      " (1, u'0.281*\"eat\" + 0.249*\"fish\" + 0.225*\"veget\" + 0.088*\"kitten\"')]\n",
      "9\n",
      "[(0, u'0.280*\"fish\" + 0.178*\"pet\" + 0.178*\"dog\" + 0.169*\"kitten\"'),\n",
      " (1, u'0.284*\"eat\" + 0.250*\"fish\" + 0.223*\"veget\" + 0.088*\"kitten\"')]\n",
      "10\n",
      "[(0, u'0.279*\"fish\" + 0.180*\"pet\" + 0.180*\"dog\" + 0.170*\"kitten\"'),\n",
      " (1, u'0.288*\"eat\" + 0.251*\"fish\" + 0.220*\"veget\" + 0.089*\"kitten\"')]\n",
      "11\n",
      "[(0, u'0.279*\"fish\" + 0.183*\"pet\" + 0.183*\"dog\" + 0.170*\"kitten\"'),\n",
      " (1, u'0.291*\"eat\" + 0.252*\"fish\" + 0.217*\"veget\" + 0.090*\"kitten\"')]\n",
      "12\n",
      "[(0, u'0.278*\"fish\" + 0.185*\"pet\" + 0.185*\"dog\" + 0.170*\"kitten\"'),\n",
      " (1, u'0.294*\"eat\" + 0.253*\"fish\" + 0.214*\"veget\" + 0.091*\"kitten\"')]\n",
      "13\n",
      "[(0, u'0.277*\"fish\" + 0.187*\"pet\" + 0.187*\"dog\" + 0.170*\"kitten\"'),\n",
      " (1, u'0.297*\"eat\" + 0.255*\"fish\" + 0.211*\"veget\" + 0.093*\"kitten\"')]\n",
      "14\n",
      "[(0, u'0.276*\"fish\" + 0.190*\"pet\" + 0.190*\"dog\" + 0.169*\"kitten\"'),\n",
      " (1, u'0.299*\"eat\" + 0.256*\"fish\" + 0.208*\"veget\" + 0.095*\"kitten\"')]\n",
      "15\n",
      "[(0, u'0.275*\"fish\" + 0.193*\"pet\" + 0.193*\"dog\" + 0.167*\"kitten\"'),\n",
      " (1, u'0.300*\"eat\" + 0.258*\"fish\" + 0.204*\"veget\" + 0.097*\"kitten\"')]\n",
      "16\n",
      "[(0, u'0.274*\"fish\" + 0.196*\"pet\" + 0.196*\"dog\" + 0.165*\"kitten\"'),\n",
      " (1, u'0.301*\"eat\" + 0.259*\"fish\" + 0.201*\"veget\" + 0.101*\"kitten\"')]\n",
      "17\n",
      "[(0, u'0.272*\"fish\" + 0.200*\"pet\" + 0.200*\"dog\" + 0.161*\"kitten\"'),\n",
      " (1, u'0.300*\"eat\" + 0.261*\"fish\" + 0.197*\"veget\" + 0.106*\"kitten\"')]\n",
      "18\n",
      "[(0, u'0.270*\"fish\" + 0.205*\"pet\" + 0.205*\"dog\" + 0.155*\"kitten\"'),\n",
      " (1, u'0.299*\"eat\" + 0.263*\"fish\" + 0.193*\"veget\" + 0.113*\"kitten\"')]\n",
      "19\n",
      "[(0, u'0.268*\"fish\" + 0.210*\"pet\" + 0.210*\"dog\" + 0.147*\"kitten\"'),\n",
      " (1, u'0.296*\"eat\" + 0.265*\"fish\" + 0.189*\"veget\" + 0.121*\"kitten\"')]\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=2, id2word = dictionary, passes=20)\n",
    "# pprint(ldamodel.print_topics(num_topics=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  u'0.279*\"eat\" + 0.276*\"fish\" + 0.175*\"kitten\" + 0.147*\"veget\" + 0.062*\"dog\" + 0.062*\"pet\"'),\n",
       " (1,\n",
       "  u'0.267*\"pet\" + 0.267*\"dog\" + 0.137*\"veget\" + 0.120*\"fish\" + 0.115*\"eat\" + 0.094*\"kitten\"')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldamodel.show_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st Topic is : 0.268*\"fish\" + 0.210*\"pet\" + 0.210*\"dog\" + 0.147*\"kitten\"\n",
      "2nd Topic is : 0.296*\"eat\" + 0.265*\"fish\" + 0.189*\"veget\" + 0.121*\"kitten\"\n",
      "\n",
      "No.1 Document : ['eat', 'fish', u'veget']\n",
      " 1st Topic: 14.67%\n",
      " 2nd Topic: 85.33%\n",
      "\n",
      "No.2 Document : ['dog', 'fish', u'pet']\n",
      " 1st Topic: 85.44%\n",
      " 2nd Topic: 14.56%\n",
      "\n",
      "No.3 Document : ['kitten', u'eat', 'fish']\n",
      " 1st Topic: 19.95%\n",
      " 2nd Topic: 80.05%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = ldamodel.print_topics(num_topics=2, num_words=4)\n",
    "print('1st Topic is : {}\\n2nd Topic is : {}\\n'.format(result[0][1], result[1][1]))\n",
    "for i, text in enumerate(texts):\n",
    "    print('No.{} Document : {}\\n 1st Topic: {:4.2f}%\\n 2nd Topic: {:4.2f}%\\n'.format(i+1,\\\n",
    "                                                                       text,\\\n",
    "                                                                       ldamodel[corpus[i]][0][1]*100,\\\n",
    "                                                                       ldamodel[corpus[i]][1][1]*100\\\n",
    "                                                                      ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
